version: '3.8'

networks:
  robot_net:
    driver: bridge

services:
  # --- The ROS 2 Control Node ---
  ros-control:
    build: ./ros_control
    container_name: ros_control
    restart: always
    networks:
      - robot_net
    # This grants the container access to your ESP32.
    # Change '/dev/ttyUSB0' to the *actual* port you found on your Jetson
    #    (e.g., /dev/ttyUSB0, /dev/ttyUSB1, or /dev/ttyACM0).
    # devices:
    #  - "/dev/ttyUSB0:/dev/ttyUSB0"
    # 'privileged' is often required for direct hardware access
    privileged: true
    volumes:
      # Mounts the host's ros_ws into the container for live development
      - ./ros_control/ros_ws:/root/ros_ws
    # This command keeps the container alive in the background
    # so you can 'exec' into it for development.
    command: tail -f /dev/null


  # --- The AI Inference Node ---
  ai-inference:
    build:
      context: ./ai_inference/jetson-inference
      dockerfile: Dockerfile
    container_name: ai_inference
    restart: always
    networks:
      - robot_net
    runtime: nvidia  # !! CRITICAL: Enables container access to the GPU
    devices:
      # Grants container access to the camera
      # !! MODIFY "/dev/video0" if your camera is different !!
      - "/dev/video0:/dev/video0"
    ports:
      # Exposes the AI's internal API port
      - "5000:5000"
    volumes:
      # Persist/download models and datasets across container rebuilds
      - ./ai_inference/jetson-inference/data:/jetson-inference/data
      # Optional: uncomment to live-develop against the whole repo (dev mode)
      # - ./ai_inference/jetson-inference:/jetson-inference
    environment:
      # Required by some Jetson containers when using GPU/graphics
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    # Notes:
    # - This service is now built from ai_inference/jetson-inference.
    # - To add more AI inference containers, create additional subdirectories
    #   under ai_inference/<model-or-app>/ with their own Dockerfile and add
    #   new services here with build.context pointing to them.