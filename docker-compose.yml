version: '3.8'

networks:
  robot_net:
    driver: bridge

services:
  # --- The ROS 2 Control Node ---
  ros-control:
    build: ./ros_control
    container_name: ros_control
    restart: always
    networks:
      - robot_net
    devices:
      # Grants container access to the ESP32
      # !! MODIFY "/dev/ttyUSB0" to match your device !!
      - "/dev/ttyUSB0:/dev/ttyUSB0"
    volumes:
      # Mounts the host's ros_ws into the container for live development
      - ./ros_control/ros_ws:/root/ros_ws
    command: >
      bash -c "source /opt/ros/humble/setup.bash && 
               source /root/ros_ws/install/setup.bash && 
               ros2 launch your_package your_launch_file.py"

  # --- The AI Inference Node ---
  ai-inference:
    build:
      context: ./ai_inference/jetson-inference
      dockerfile: Dockerfile
    container_name: ai_inference
    restart: always
    networks:
      - robot_net
    runtime: nvidia  # !! CRITICAL: Enables container access to the GPU
    devices:
      # Grants container access to the camera
      # !! MODIFY "/dev/video0" if your camera is different !!
      - "/dev/video0:/dev/video0"
    ports:
      # Exposes the AI's internal API port
      - "5000:5000"
    volumes:
      # Persist/download models and datasets across container rebuilds
      - ./ai_inference/jetson-inference/data:/jetson-inference/data
      # Optional: uncomment to live-develop against the whole repo (dev mode)
      # - ./ai_inference/jetson-inference:/jetson-inference
    environment:
      # Required by some Jetson containers when using GPU/graphics
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
    # Notes:
    # - This service is now built from ai_inference/jetson-inference.
    # - To add more AI inference containers, create additional subdirectories
    #   under ai_inference/<model-or-app>/ with their own Dockerfile and add
    #   new services here with build.context pointing to them.